# -*- coding: utf-8 -*-
"""sentacecreationdeslexia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wuPsomWBBmldhvYsie8cXdRxDjz7rL6I
"""

!unzip /content/sample_data/ncert_pdfs-20260214T193707Z-1-001.zip -d /content/sample_data/fdats

pip install pymupdf

pip install nltk

pip install fitz

import fitz  # PyMuPDF
import nltk
import os

# Download tokenizer
nltk.download('punkt')
nltk.download('punkt_tab') # Download 'punkt_tab' as well

def extract_sentences_from_pdf(pdf_path):
    sentences = []
    try:
        doc = fitz.open(pdf_path)
        full_text = ""

        for page in doc:
            full_text += page.get_text()

        doc.close()

        # Remove extra spaces and newlines
        full_text = " ".join(full_text.split())

        # Split into sentences
        sentences = nltk.sent_tokenize(full_text)

    except Exception as e:
        print(f"Error processing {pdf_path}: {e}")

    return sentences


def create_ncert_corpus(input_folder, output_file):
    all_sentences = []

    for filename in os.listdir(input_folder):
        if filename.endswith(".pdf"):
            print(f"Processing {filename}...")
            path = os.path.join(input_folder, filename)
            all_sentences.extend(extract_sentences_from_pdf(path))

    # Filter small fragments
    filtered_sentences = [s for s in all_sentences if len(s) > 15]

    with open(output_file, 'w', encoding='utf-8') as f:
        for sentence in filtered_sentences:
            f.write(sentence + "\n")

    print(f"Done! Total valid sentences extracted: {len(filtered_sentences)}")


# Run
create_ncert_corpus('/content/sample_data/fdats/ncert_pdfs', 'ncert_sentences.txt')